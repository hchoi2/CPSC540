import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import catboost as cat
import shap

# Load data
data = pd.read_csv('./Data/WineQT.csv')
X = data.iloc[:, 0:11]
y = data.iloc[:, 11]
print("Label Counts:")
print(y.value_counts())

# Print the label counts
data.info()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=58)
print("Label Train Counts:")
print(y_train.value_counts())
print("Label Test Counts:")
print(y_test.value_counts())

# Baseline Model
most_frequent_class = y_train.mode()[0]
baseline_predictions = np.full(shape=y_test.shape, fill_value=most_frequent_class)
baseline_accuracy = accuracy_score(y_test, baseline_predictions)
baseline_error = 1 - baseline_accuracy

print(f'Baseline Accuracy: {baseline_accuracy}')
print(f'Baseline Error Rate: {baseline_error}')

X_train_scaled = X_train
X_test_scaled = X_test

param_grid = {
    'iterations': [200],
    'learning_rate': [0.2],
    'max_depth': [4],
    'l2_leaf_reg': [0.01],
    'early_stopping_rounds': [20],
}

cbr_model = cat.CatBoostClassifier(verbose=0, objective='MultiClass')

# Use StratifiedKFold for cross-validation
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Create empty lists to store training and validation losses
train_losses = []
val_losses = []

# Iterate over hyperparameter combinations
for params in ParameterGrid(param_grid):
    print("Testing hyperparameters:", params)

    # Create the CatBoost model with current hyperparameters
    cbr_model.set_params(**params)

    # Lists to store losses for each fold
    fold_train_losses = []
    fold_val_losses = []

    # Perform cross-validation
    for train_index, val_index in cv.split(X_train_scaled, y_train):
        X_train_fold, X_val_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Fit the model
        cbr_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), early_stopping_rounds=20, verbose=0)

        # Get the evaluation history for the current fold
        eval_history = cbr_model.evals_result_

        # Append training and validation losses for each iteration
        fold_train_losses.append(eval_history['learn']['MultiClass'])
        fold_val_losses.append(eval_history['validation']['MultiClass'])

    # Calculate average training and validation losses across folds for each iteration
    avg_train_losses = np.mean(fold_train_losses, axis=0)
    avg_val_losses = np.mean(fold_val_losses, axis=0)

    # Append average losses to the overall lists
    train_losses.append(avg_train_losses)
    val_losses.append(avg_val_losses)

    print("Average Cross-Validation Training Loss:", avg_train_losses[-1])
    print("Average Cross-Validation Validation Loss:", avg_val_losses[-1])
    print()

# Plot the learning curves for each set of hyperparameters
plt.figure(figsize=(10, 6))
for i, params in enumerate(ParameterGrid(param_grid)):
    plt.plot(train_losses[i], label=f'Training Loss - {params}')
    plt.plot(val_losses[i], label=f'Validation Loss - {params}')

plt.title('Learning Curves for Different Hyperparameter Combinations')
plt.xlabel('Iterations')
plt.ylabel('MultiClass Loss')
plt.legend()
plt.show()